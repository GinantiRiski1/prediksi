# -*- coding: utf-8 -*-
"""rekomendasi.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1oYIj_P3PBqXB_Nfwa3f6wwmJ_DVtYAjw

#Data Understanding
"""

from google.colab import drive
import pandas as pd
import numpy as np

#1. Menghubungkan ke Drive Karena Dataset Berada Didrive
drive.mount('/content/drive')

"""**Penjelasan Dataset**


   








**Hubungan antar tabel:**
- `movieId` adalah kunci utama yang menghubungkan semua dataset ini.
- `ratings` dan `tags` berisi interaksi user terhadap film (`movieId`).
- `links` menghubungkan `movieId` ke ID film di database eksternal (IMDb, TMDb).
- `movies` adalah daftar utama film yang akan direkomendasikan.

**terdapat 4 dataset berbeda yang masing" memiliki 1 variabel yang saling terhubung yaitu `movieId`, karena dataset terlalu banyak hingga ratusan ribu dan memperlambat pemrosesan, kita hanya mengggunakan 10k baris pertama disetiap data**

**hasilnya :**
menunjukkan bahwa data yang tersedia di movie dan link tidak ada baris yang kosong, namun di ratings dan tags kemungkinan ada data yg kosong karena dari 10k data hanya sekitar 2-3k data saja yang terdeteksi.
"""

#2. Load Dataset
movies = pd.read_csv('/content/drive/MyDrive/rekomendasi/movies.csv')
links = pd.read_csv('/content/drive/MyDrive/rekomendasi/links.csv')
ratings = pd.read_csv('/content/drive/MyDrive/rekomendasi/ratings.csv')
tags = pd.read_csv('/content/drive/MyDrive/rekomendasi/tags.csv')

#3. Mengambil Hanya 10000 Baris Data Pertama disetiap Variabel
movies1 = movies.head(10000)
links1 = links.head(10000)
ratings1 = ratings.head(10000)
tags1 = tags.head(10000)

#4. Menghitung Jumlah Data yang Diambil
print('Jumlah data movie yang tersedia: ', len(movies1.movieId.unique()))
print('Jumlah data links yang tersedia: ', len(links1.movieId.unique()))
print('Jumlah data ratings yang tersedia: ', len(ratings1.movieId.unique()))
print('Jumlah data tags yang tersedia: ', len(tags1.movieId.unique()))

"""#Univariate Exploratory Data Analysis

1. **movies.csv**
   - Berisi daftar film.
   - Kolom:
     - `movieId`: ID unik untuk setiap film.
     - `title`: Judul film.
     - `genres`: Genre dari film tersebut.
     
hasil pengecekan bahwa tidak ada missing values didalamnya, namun genres menunjukkan hanya sekitar 780 data unik, yang artinya ada judul film yang memiliki genre yang sama didalam dataset
"""

# mengecek informasi pada dataset movies
movies1.info()

print('Banyak jenis genre: ', len(movies1.genres.unique()))
print('Jenis genre yang tersedia: ', movies1.genres.unique())

"""2. **links.csv**
   - Berisi tautan (link) dari `movieId` ke ID film di situs lain (IMDb, TMDb).
   - Kolom:
     - `movieId`: ID film (sama dengan di movies.csv).
     - `imdbId`: ID film di IMDb.
     - `tmdbId`: ID film di TMDb.

`movieId` dan `imdbId` menunjukkan data yg unik sebanyak 10k data, artinya tidak ada yg duplikat, dan di `tmdbId` terdapat nilai missing values sebanyak 26 data
"""

# mengecek informasi dataset links
links1.info()

print('Banyak movieId unik di links: ', len(links1.movieId.unique()))
print('Banyak imdbId unik: ', len(links1.imdbId.unique()))
print('Banyak tmdbId unik: ', len(links1.tmdbId.unique()))

"""3. **ratings.csv**
   - Berisi data rating yang diberikan user terhadap film.
   - Kolom:
     - `userId`: ID pengguna.
     - `movieId`: ID film yang dirating.
     - `rating`: Nilai rating (skala biasanya 0.5 - 5.0).
     - `timestamp`: Waktu rating diberikan.

dari data tidak terdapat missing values, banyak rating yang diberikan ada 10, yaitu yang berkisar dari 1-5
"""

# mengecek informasi dataset ratings
ratings1.info()

print('Banyak jenis rating yang diberikan: ', len(ratings1.rating.unique()))
print('Jenis rating yang tersedia: ', ratings1.rating.unique())

ratings.describe()

"""4. **tags.csv**
   - Berisi tag/kata kunci yang diberikan user ke film.
   - Kolom:
     - `userId`: ID pengguna.
     - `movieId`: ID film yang diberi tag.
     - `tag`: Kata kunci/tag.
     - `timestamp`: Waktu tag diberikan.

tidak terdapat missing values, namunbanyak tipe tag hanya 2880, artinya ada movieId yang memiliki tag yg sama
"""

tags1.info()

print('Banyak tipe tag: ', len(tags1.tag.unique()))
print('Tipe tag yang tersedia: ', tags1.tag.unique())

"""**Kesimpulan Pemeriksaan**

- Dataset memiliki **66 pengguna** yang memberikan rating pada **10000 film**.
- Terdapat total **10000 data rating** yang menunjukkan interaksi antara pengguna dan film.
"""

print('Jumlah userId: ', len(ratings1.userId.unique()))
print('Jumlah movieId: ', len(movies1.movieId.unique()))
print('Jumlah data rating: ', len(ratings1))

"""# Data Preprocessing

**setelah semua file digabung, kita memiliki 11959 Movie yang unik.**
"""

#1. Menggabungkan seluruh movieId pada kategori Movie
movie_all = np.concatenate((
    movies1.movieId.unique(),
    links1.movieId.unique(),
    ratings1.movieId.unique(),
    tags1.movieId.unique()
))

#2 Mengurutkan data dan menghapus data yang sama
movie_all = np.sort(np.unique(movie_all))

print('Jumlah seluruh data movie berdasarkan movieId: ', len(movie_all))

"""Kita memiliki 206 data pengguna dari 11959 movie yang memiliki rating.
Jadi, kesimpulannya:

Banyak pengguna aktif yang memberi rating pada beragam film.

Meskipun ada banyak film, tidak semua film mendapat perhatian dari setiap pengguna, mengingat ada kemungkinan rating yang sparse (misalnya, beberapa film hanya dirating oleh sedikit pengguna).
"""

#3. Menggabungkan seluruh userId
user_all = np.concatenate((
    ratings1.userId.unique(),
    tags1.userId.unique()
))

#4 Menghapus data yang sama kemudian mengurutkannya
user_all = np.sort(np.unique(user_all))

print('Jumlah seluruh user: ', len(user_all))

"""Disini kita coba menggabungkan seluruh record pada data, namun hanya berdasarkan movieId, sehingga hasil dari penggabunan akan menghasilkan setiap record pada dataset kecuali `movieId`, dan terlihat pada table banyak sekali yang berisi `NaN` artinya ada missing values didalamnya"""

#5. Merge data berdasarkan movieId
movie_info = pd.merge(movies1, links1, on='movieId', how='left')
movie_info = pd.merge(movie_info, tags1, on='movieId', how='left')
movies = pd.merge(ratings1, movie_info, on='movieId', how='left')

#6. Hapus kolom duplikat userId_y dan timestamp_y
movies = movies.drop(columns=['userId_y', 'timestamp_y'])

#7. Rename kolom supaya lebih rapi
movies = movies.rename(columns={'userId_x': 'userId', 'timestamp_x': 'timestamp'})

#8. (Opsional) Simpan hasilnya
movies.to_csv('/content/drive/MyDrive/rekomendasi/movies_merged_fix.csv', index=False)

#9. Tampilkan hasil akhirnya
movies

"""banyak sekali yang memiliki missing values yaitu `title`, `genres`, `imdbId`, `tmdbId` dan `tag`"""

# 10. cek banyak data yang missing values
movies.isnull().sum()

"""kita hanya memilih variabel yang kita akan gunakan nantinya dan terlihat masih ada missing values yang kita atasi nanti"""

#11. mengambil variabel UserId, movieId, rating dan timestamp pada dataframe movies dan disimpan di variabel all_movie_rate
all_movie_rate = movies[['userId', 'movieId', 'rating', 'timestamp']]

#12. Menggabungkan all movie_rate dengan dataframe movies berdasarkan movieId
all_movie = pd.merge(all_movie_rate, movies[['movieId','title','genres']], on='movieId', how='left')

# Print dataframe all_movie_name
all_movie

"""# Data Preparation

terdapat missing values pada variabel `title` dan `genres` sebanyak 7843
"""

#1. Mengecek missing value pada dataframe all_movie
all_movie.isnull().sum()

"""kita akan menghapus missing values tersebut, sehingga datanya bersih sekarang"""

#2. Membersihkan missing value dengan fungsi dropna()
all_movie_clean = all_movie.dropna()
all_movie_clean

"""disini kita akan mengurutkan data berdasrkan `movieId` secara ascending"""

#3. Menyamakan Jenis Genre
# Mengurutkan movie berdasarkan movieId kemudian memasukkannya ke dalam variabel fix_movie
fix_movie = all_movie_clean.sort_values('movieId', ascending=True)
fix_movie

"""disini kita akan mengecek jumlah `movieId` yang unik dan hanya terdapat 2796 data, berarti banyak duplikat data di `movieId`"""

#4. Mengecek berapa jumlah fix_movie
len(fix_movie.movieId.unique())

"""hasil pengecekkan menunjukkan jenis genres yang ada pada variabel fix_movie"""

#5. Mengecek kategori movie yang unik
fix_movie.genres.unique()

"""disini kita akan menyalin variabel fix_movie kedalam variabel preparation dan mengurutkannya secara ascending berdasarkan `movieId`"""

#6. Membuat variabel preparation yang berisi dataframe fix_movie kemudian mengurutkan berdasarkan movieId
preparation = fix_movie
preparation.sort_values('movieId')

"""disini kita akan menghapus data duplikat berdasarkan `movieId`, sehingga datanya sekarang sudah bersih"""

#7. Membuang data duplikat pada variabel preparation
preparation = preparation.drop_duplicates('movieId')
preparation

"""dari sini kita melihat bahwa rating 3 dan 4 cenderung lebih banyak dalam dataset"""

# Set tema visualisasi
import seaborn as sns
import matplotlib.pyplot as plt
sns.set(style="whitegrid")

# Distribusi Rating Film
plt.figure(figsize=(10, 6))
sns.histplot(preparation['rating'], kde=True, color='blue', bins=30)
plt.title('Distribusi Rating Film', fontsize=16)
plt.xlabel('Rating', fontsize=12)
plt.ylabel('Frekuensi', fontsize=12)
plt.show()

"""disini kita melihat rata" rating setiap genre berada diatas 3"""

# Rata-rata Rating per Genre
# Pertama, kita pecah genre menjadi list
preparation['genre_list'] = preparation['genres'].str.split('|')

# Membuat dataframe baru yang berisi rata-rata rating per genre
genre_ratings = preparation.explode('genre_list').groupby('genre_list')['rating'].mean().sort_values(ascending=False)

plt.figure(figsize=(12, 6))
genre_ratings.plot(kind='bar', color='green')
plt.title('Rata-rata Rating per Genre', fontsize=16)
plt.xlabel('Genre', fontsize=12)
plt.ylabel('Rata-rata Rating', fontsize=12)
plt.xticks(rotation=90)
plt.show()

"""disini kita akan membuat `movieId`, `title`, dan `genres` kedalam bentuk list, dan hanya terdapat hanya 2796 data setelah dibersihkan"""

#8. Mengonversi data series ‘movieId’ menjadi dalam bentuk list
movie_id = preparation['movieId'].tolist()

#9. Mengonversi data series ‘title’ menjadi dalam bentuk list
movie_title = preparation['title'].tolist()

#10. Mengonversi data series ‘genres’ menjadi dalam bentuk list
movie_genres = preparation['genres'].tolist()

print(len(movie_id))
print(len(movie_title))
print(len(movie_genres))

"""kita buat list tersebut kedalam dictionary"""

# Membuat dictionary untuk data ‘movie_id’, ‘movie_title’, dan ‘genres’
movie_new = pd.DataFrame({
    'id': movie_id,
    'title': movie_title,
    'genres': movie_genres
})
movie_new

"""# Model Development dengan Content Based Filtering

kita copy variabel movie_new kedalam variabel data untuk dibangun model berdasarkan content based filtering
"""

data = movie_new
data.sample(5)

"""disini kita akan melakukan vektorisasi pada kolom genres sehingga kita dapat pisahkan dengan array, karena setiap movie memiliki jenis genres lebih dari 1"""

#TF-IDF Vectorizer
from sklearn.feature_extraction.text import TfidfVectorizer

# Inisialisasi TfidfVectorizer
tf = TfidfVectorizer()

# Melakukan perhitungan idf pada data genres
tf.fit(data['genres'])

# Mapping array dari fitur index integer ke fitur nama
tf.get_feature_names_out()

"""kita transformasikan vektor diatas kedalam bentuk matriks, dan menunjukkan ada 2796 data dengan 21 genres"""

# Melakukan fit lalu ditransformasikan ke bentuk matrix
tfidf_matrix = tf.fit_transform(data['genres'])

# Melihat ukuran matrix tfidf
tfidf_matrix.shape

# Mengubah vektor tf-idf dalam bentuk matriks dengan fungsi todense()
tfidf_matrix.todense()

"""kita akan buat dataframe hasil tf-idf matrix, disini terlihat genres drama yaitu French LieuPushing Tin (1999) memiliki 1.0 dan seterusnya.
ini adalah bentuk korelasi antar title dan genres
"""

# Membuat dataframe untuk melihat tf-idf matrix
# Kolom diisi dengan jenis genres
# Baris diisi dengan title movie

pd.DataFrame(
    tfidf_matrix.todense(),
    columns=tf.get_feature_names_out(),
    index=data.title
).sample(21, axis=1).sample(10, axis=0)

"""Sekarang, kita akan menghitung derajat kesamaan (similarity degree) antar movie dengan teknik cosine similarity. Di sini, kita menggunakan fungsi cosine_similarity dari library sklearn."""

#Cosine Similarity
from sklearn.metrics.pairwise import cosine_similarity

# Menghitung cosine similarity pada matrix tf-idf
cosine_sim = cosine_similarity(tfidf_matrix)
cosine_sim

"""Dengan cosine similarity, kita berhasil mengidentifikasi kesamaan antara satu movie dengan movie lainnya. Shape (2796, 2796) merupakan ukuran matriks similarity dari data yang kita miliki. Berdasarkan data yang ada, matriks di atas sebenarnya berukuran 2796 restoran x 2796 movie (masing-masing dalam sumbu X dan Y). Artinya, kita mengidentifikasi tingkat kesamaan pada 2796 title movie. Tapi tentu kita tidak bisa menampilkan semuanya. Oleh karena itu, kita hanya memilih 10 restoran pada baris vertikal dan 5 restoran pada sumbu horizontal seperti pada contoh di bawah. <br>

seperti pada output Legally Blonde (2001) dan Girl Next Door, The (2004) memiliki kesamaan dengan Milk Money (1994), Where the Day Takes You (1992) dan Doors, The (1991) memiliki kesamaan dengan Nell (1994) dan seterusnya
"""

# Membuat dataframe dari variabel cosine_sim dengan baris dan kolom berupa nama resto
cosine_sim_df = pd.DataFrame(cosine_sim, index=data['title'], columns=data['title'])
print('Shape:', cosine_sim_df.shape)

# Melihat similarity matrix pada setiap resto
cosine_sim_df.sample(5, axis=1).sample(10, axis=0)

"""disini kita membuat fungsi untuk mendapatkan top 5 rekemondasi berdasarkan kesamaan genres"""

#Mendapatkan Rekomendasi
def movie_recommendations(title, similarity_data=cosine_sim_df, items=data[['title', 'genres']], k=5):
    """
    Rekomendasi Movie berdasarkan kemiripan dataframe

    Parameter:
    ---
    title : tipe data string (str)
                title movie (index kemiripan dataframe)
    similarity_data : tipe data pd.DataFrame (object)
                      Kesamaan dataframe, simetrik, dengan movie sebagai
                      indeks dan kolom
    items : tipe data pd.DataFrame (object)
            Mengandung kedua nama dan fitur lainnya yang digunakan untuk mendefinisikan kemiripan
    k : tipe data integer (int)
        Banyaknya jumlah rekomendasi yang diberikan
    ---


    Pada index ini, kita mengambil k dengan nilai similarity terbesar
    pada index matrix yang diberikan (i).
    """


    # Mengambil data dengan menggunakan argpartition untuk melakukan partisi secara tidak langsung sepanjang sumbu yang diberikan
    # Dataframe diubah menjadi numpy
    # Range(start, stop, step)
    index = similarity_data.loc[:,title].to_numpy().argpartition(
        range(-1, -k, -1))

    # Mengambil data dengan similarity terbesar dari index yang ada
    closest = similarity_data.columns[index[-1:-(k+2):-1]]

    # Drop title agar nama resto yang dicari tidak muncul dalam daftar rekomendasi
    closest = closest.drop(title, errors='ignore')

    return pd.DataFrame(closest).merge(items).head(k)

"""disini kita akan cek title movie Jumanji (1995) dalam data, dan terdeteksi bahwa movie itu bergenre Adventure|Children|Fantasy"""

data[data.title.eq('Jumanji (1995)')]

"""kemudian kita akan mengambil 5 rekomendasi film yg sama dengan genres yg sama"""

# Mendapatkan rekomendasi movie yang mirip dengan Jumanji (1995)
movie_recommendations('Jumanji (1995)')

"""hasil evaluasi rekomendasi menunjukkan bahwa 5 movie tersebut adalah sama dengan genre Jumanji (1995), dengan hasil precision@10 adalah 100%, artinya fungsi ini berjalan dengan baik"""

TP = 5
FP = 0
precision = TP / (TP + FP)
print(f"Precision@10: {precision:.0%}")

"""# Model Development dengan Collaborative Filtering"""

#Data Understanding
# Import library
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from pathlib import Path
import matplotlib.pyplot as plt

"""disini kita hanya akan menggunakan UserId, movieId, rating dan timestamp untuk membangun Collaboratiove Filtering berdasarkan User Based Filtering"""

# Membaca dataset

df = movies[['userId', 'movieId', 'rating', 'timestamp']]
df

#Data Preparation

"""disini kita melakukan perubahan userId menjadi list, lalu kita encoding"""

# Mengubah userID menjadi list tanpa nilai yang sama
user_ids = df['userId'].unique().tolist()
print('list userId: ', user_ids)

# Melakukan encoding userId
user_to_user_encoded = {x: i for i, x in enumerate(user_ids)}
print('encoded userId : ', user_to_user_encoded)

# Melakukan proses encoding angka ke ke userId
user_encoded_to_user = {i: x for i, x in enumerate(user_ids)}
print('encoded angka ke userId: ', user_encoded_to_user)

# Mengubah movieId menjadi list tanpa nilai yang sama
movie_ids = df['movieId'].unique().tolist()

# Melakukan proses encoding movieId
movie_to_movie_encoded = {x: i for i, x in enumerate(movie_ids)}

# Melakukan proses encoding angka ke movieId
movie_encoded_to_movie= {i: x for i, x in enumerate(movie_ids)}

"""hasil encoding tersebut akan kita mapping ke dataframe movie"""

# Mapping userId ke dataframe user
df['user'] = df['userId'].map(user_to_user_encoded)

# Mapping movieId ke dataframe movie
df['movie'] = df['movieId'].map(movie_to_movie_encoded)

"""hasil output menunjukkan bahwa ada 66 user yang memberikan rating ke 3965 movie"""

# Mendapatkan jumlah user
num_users = len(user_to_user_encoded)
print(num_users)

# Mendapatkan jumlah movie
num_resto = len(movie_encoded_to_movie)
print(num_resto)

# Mengubah rating menjadi nilai float
df['rating'] = df['rating'].values.astype(np.float32)

# Nilai minimum rating
min_rating = min(df['rating'])

# Nilai maksimal rating
max_rating = max(df['rating'])

print('Number of User: {}, Number of Resto: {}, Min Rating: {}, Max Rating: {}'.format(
    num_users, num_resto, min_rating, max_rating
))

"""kita akan coba mengacak dataset"""

# Mengacak dataset
df = df.sample(frac=1, random_state=42)
df

"""dan kita simpan hasil data yg acak tersebut kedalam rating_final.csv untuk testing model nantinya"""

# Pilih hanya kolom userId, movieId, dan rating
rating_final = df[['userId', 'movieId', 'rating']]

# Simpan ke file CSV
rating_final.to_csv('/content/rating_final.csv', index=False)

print('rating_final.csv berhasil dibuat!')

"""disini hasil data rating akan kita split menjadi 80% untuk train dan 20% untuk data validasi"""

# Membuat variabel x untuk mencocokkan data user dan movie menjadi satu value
x = df[['user', 'movie']].values

# Membuat variabel y untuk membuat rating dari hasil
y = df['rating'].apply(lambda x: (x - min_rating) / (max_rating - min_rating)).values

# Membagi menjadi 80% data train dan 20% data validasi
train_indices = int(0.8 * df.shape[0])
x_train, x_val, y_train, y_val = (
    x[:train_indices],
    x[train_indices:],
    y[:train_indices],
    y[train_indices:]
)

print(x, y)

"""mengambil uderId dan movieId unik yang akan disimpan divariabel baru yaitu num_users dan num_movie"""

num_users = df['userId'].nunique()
num_movie = df['movieId'].nunique()

"""

`RecommenderNet` adalah model rekomendasi berbasis **embedding** menggunakan TensorFlow dan Keras, yang mempelajari hubungan antara **user** dan **movie** berdasarkan rating.

1. `__init__` (Constructor)

- Membuat layer embedding untuk user dan movie.
- Membuat layer bias untuk user dan movie.

**Parameter:**
- `num_users` : jumlah user unik.
- `num_movie` : jumlah movie unik.
- `embedding_size` : ukuran vektor embedding.
- `**kwargs` : argumen tambahan untuk model.

**Layer yang dibuat:**
- `user_embedding` : mengubah userId menjadi vektor.
- `user_bias` : bias untuk setiap user.
- `movie_embedding` : mengubah movieId menjadi vektor.
- `movie_bias` : bias untuk setiap movie.

2. `call` (Forward Pass)

- Input: array berisi pasangan `(userId, movieId)`.
- Output: skor prediksi kecocokan user terhadap movie.

**Proses:**
1. Mengambil embedding user (`user_vector`) dari kolom ke-0.
2. Mengambil bias user (`user_bias`) dari kolom ke-0.
3. Mengambil embedding movie (`movie_vector`) dari kolom ke-1.
4. Mengambil bias movie (`movie_bias`) dari kolom ke-1.
5. Menghitung **dot product** antara `user_vector` dan `movie_vector`.
6. Menambahkan `user_bias` dan `movie_bias` ke hasil dot product.
7. Menggunakan fungsi aktivasi **sigmoid** untuk mengubah skor ke rentang 0-1.

3. Diagram Alur Proses

```
Input (userId, movieId)
      ↓
Embedding user → user_vector
Embedding movie → movie_vector
      ↓
Hitung dot product (user_vector . movie_vector)
      ↓
+ bias user
+ bias movie
      ↓
Sigmoid Activation
      ↓
Output (prediksi rating)
```

4. Catatan Tambahan
- **Inisialisasi bobot** menggunakan `he_normal`.
- **Regularisasi L2** diterapkan untuk menghindari overfitting.
- Aktivasi sigmoid cocok untuk menghasilkan output berbentuk skor 0-1.

"""

#Proses Training
class RecommenderNet(tf.keras.Model):

  # Insialisasi fungsi
  def __init__(self, num_users, num_movie, embedding_size, **kwargs):
    super(RecommenderNet, self).__init__(**kwargs)
    self.num_users = num_users
    self.num_movie = num_movie
    self.embedding_size = embedding_size
    self.user_embedding = layers.Embedding( # layer embedding user
        num_users,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.user_bias = layers.Embedding(num_users, 1) # layer embedding user bias
    self.movie_embedding = layers.Embedding( # layer embeddings movie
        num_movie,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.movie_bias = layers.Embedding(num_movie, 1) # layer embedding movie bias

  def call(self, inputs):
    user_vector = self.user_embedding(inputs[:,0]) # memanggil layer embedding 1
    user_bias = self.user_bias(inputs[:, 0]) # memanggil layer embedding 2
    movie_vector = self.movie_embedding(inputs[:, 1]) # memanggil layer embedding 3
    movie_bias = self.movie_bias(inputs[:, 1]) # memanggil layer embedding 4

    dot_user_movie = tf.tensordot(user_vector, movie_vector, 2)

    x = dot_user_movie + user_bias + movie_bias

    return tf.nn.sigmoid(x) # activation sigmoid

"""
Setelah membuat arsitektur `RecommenderNet`, langkah berikutnya adalah **menginisialisasi model** dan **menyiapkan proses training** melalui compile.

1. Inisialisasi Model

```python
model = RecommenderNet(num_users, num_movie, 50)
```

**Penjelasan:**
- Membuat objek dari class `RecommenderNet`.
- **Parameter:**
  - `num_users` : jumlah user unik yang sudah diencoding.
  - `num_movie` : jumlah movie unik yang sudah diencoding.
  - `50` : ukuran **embedding vector** yang dipakai untuk merepresentasikan user dan movie.

Artinya, baik user maupun movie akan direpresentasikan dalam **vektor berdimensi 50**.

---

2. Compile Model

```python
model.compile(
    loss = tf.keras.losses.BinaryCrossentropy(),
    optimizer = keras.optimizers.Adam(learning_rate=0.001),
    metrics=[tf.keras.metrics.RootMeanSquaredError()]
)
```

**Penjelasan:**
- **Loss function**:
  - Menggunakan `BinaryCrossentropy` karena target prediksi adalah **rating antara 0 dan 1** (output sigmoid).
- **Optimizer**:
  - `Adam` optimizer dipilih karena cepat dalam konvergensi.
  - Learning rate ditetapkan sebesar `0.001`.
- **Metrics**:
  - `RootMeanSquaredError (RMSE)` digunakan untuk mengukur performa prediksi, lebih sensitif terhadap error besar.

---

3. Alur Singkat Proses Training

```
Data Input (user, movie) + rating →
Model RecommenderNet →
Hitung prediksi rating →
Loss dihitung dengan Binary Crossentropy →
Optimizer Adam memperbarui bobot →
Monitoring metric RMSE selama training
```

---

Setelah tahap ini, model siap untuk **proses training** menggunakan `model.fit()`.
"""

model = RecommenderNet(num_users, num_movie, 50) # inisialisasi model

# model compile
model.compile(
    loss = tf.keras.losses.BinaryCrossentropy(),
    optimizer = keras.optimizers.Adam(learning_rate=0.001),
    metrics=[tf.keras.metrics.RootMeanSquaredError()]
)

"""Setelah model `RecommenderNet` berhasil diinisialisasi dan dicompile, tahap selanjutnya adalah **training model** menggunakan data yang sudah disiapkan sebelumnya.

1. Mulai Training Model

```python
history = model.fit(
    x = x_train,
    y = y_train,
    batch_size = 8,
    epochs = 100,
    validation_data = (x_val, y_val)
)
```

**Penjelasan Parameter:**
- `x_train`: data input training (berisi pasangan user dan movie yang sudah diencoding).
- `y_train`: label target training (berisi rating dari user ke movie).
- `batch_size=8`: ukuran batch adalah 8 sampel per iterasi training.
- `epochs=100`: model akan dilatih selama 100 kali melewati seluruh dataset (100 * iterasi training).
- `validation_data=(x_val, y_val)`: dataset validasi untuk memonitor performa model di data yang tidak dilatih.

---

2. Mekanisme Selama Training
- Model belajar memprediksi hubungan antara user dan movie berdasarkan rating.
- Loss dan RMSE diukur di setiap epoch, baik untuk data training maupun data validasi.
- Hasil training disimpan di variabel `history` yang dapat digunakan untuk visualisasi grafik loss dan RMSE.

---

3. Tips Tambahan
- Jika model overfitting (val_loss naik terus), bisa dicoba:
  - Menurunkan `batch_size`.
  - Mengurangi `epochs`.
  - Menambahkan `Dropout` atau regularisasi.
  - Menggunakan EarlyStopping callback.

---

Setelah training selesai, kamu bisa lanjut ke **evaluasi** atau **membuat rekomendasi** berdasarkan model ini.

"""

# Memulai training

history = model.fit(
    x = x_train,
    y = y_train,
    batch_size = 8,
    epochs = 100,
    validation_data = (x_val, y_val)
)

"""Setelah proses training model `RecommenderNet` selama **100 epoch**, diperoleh hasil berikut:

- **Root Mean Squared Error (RMSE) pada data training** = **0.1591**
- **Root Mean Squared Error (RMSE) pada data validasi** = **0.1783**

---

Apa Artinya?

1. **RMSE Training (0.1591)** menunjukkan rata-rata kesalahan prediksi model terhadap data training cukup kecil.
2. **RMSE Validation (0.1783)** juga masih cukup rendah dan **dekat dengan RMSE Training**, yang berarti:
   - Model **tidak mengalami overfitting** signifikan (perbedaan error training dan validasi kecil).
   - Model mampu **mengeneralisasi** data yang belum pernah dilihat dengan cukup baik.

---

Kesimpulan
- Model `RecommenderNet` yang dilatih sudah **cukup baik** dalam memahami pola interaksi antara user dan movie.
- Dengan error sekitar **0.1783** di data validasi, model dapat dipakai untuk melakukan **prediksi rekomendasi** yang akurat untuk pengguna baru.

Model siap digunakan untuk **membuat sistem rekomendasi film** berbasis prediksi rating user terhadap movie yang belum mereka tonton.


"""

plt.plot(history.history['root_mean_squared_error'])
plt.plot(history.history['val_root_mean_squared_error'])
plt.title('model_metrics')
plt.ylabel('root_mean_squared_error')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

"""- **`movie_df`** adalah dataframe berisi seluruh daftar movie.
- **`df`** adalah dataframe berisi data rating user terhadap movie.

Langkah-langkah:
1. Mengambil **satu user secara acak** dari dataset `rating_final.csv`.
2. Menentukan daftar **movie yang sudah dikunjungi** (diberi rating) oleh user tersebut.
3. Menggunakan operator **bitwise (~)** untuk memilih **movie yang belum dikunjungi**.
4. Melakukan **encoding** pada daftar movie yang belum dikunjungi.
5. Menyiapkan array **user_movie_array** untuk prediksi rating terhadap semua movie yang belum dikunjungi user.

Output: Data siap untuk digunakan dalam **prediksi rekomendasi** menggunakan model!


"""

#Mendapatkan Rekomendasi Movie
movie_df = movie_new
df = pd.read_csv('rating_final.csv')

# Mengambil sample user
user_id = df.userId.sample(1).iloc[0]
movie_visited_by_user = df[df.userId == user_id]

# Operator bitwise (~), bisa diketahui di sini https://docs.python.org/3/reference/expressions.html
movie_not_visited = movie_df[~movie_df['id'].isin(movie_visited_by_user.userId.values)]['id']
resto_not_visited = list(
    set(movie_not_visited)
    .intersection(set(movie_to_movie_encoded.keys()))
)

movie_not_visited = [[movie_to_movie_encoded.get(x)] for x in movie_not_visited]
user_encoder = user_to_user_encoded.get(user_id)
user_movie_array = np.hstack(
    ([[user_encoder]] * len(movie_not_visited), movie_not_visited)
)

"""Prediksi dan Menampilkan Rekomendasi Movie

- **`ratings`**: Model memprediksi skor rating untuk setiap movie yang belum dikunjungi user.
- **`top_ratings_indices`**: Mengambil indeks 10 movie dengan prediksi skor tertinggi.
- **`recommended_movie_ids`**: Mengubah indeks movie hasil prediksi menjadi movieId asli.

Langkah-langkah:
1. **Prediksi** rating movie yang belum dikunjungi.
2. **Ambil** 10 movie dengan rating tertinggi sebagai rekomendasi.
3. **Tampilkan**:
   - 5 movie favorit yang pernah dirating tinggi oleh user.
   - 10 rekomendasi movie terbaik untuk user berdasarkan prediksi model.
Hasil: Daftar rekomendasi movie yang dipersonalisasi untuk setiap user!


"""

ratings = model.predict(user_movie_array).flatten()

top_ratings_indices = ratings.argsort()[-10:][::-1]
recommended_movie_ids = [
    movie_encoded_to_movie.get(movie_not_visited[x][0]) for x in top_ratings_indices
]

print('Showing recommendations for users: {}'.format(user_id))
print('===' * 9)
print('Movie with high ratings from user')
print('----' * 8)

top_movie_user = (
    movie_visited_by_user.sort_values(
        by = 'rating',
        ascending=False
    )
    .head(5)
    .movieId.values
)

movie_df_rows = movie_df[movie_df['id'].isin(top_movie_user)]
for row in movie_df_rows.itertuples():
    print(row.title, ':', row.genres)

print('----' * 8)
print('Top 10 movie recommendation')
print('----' * 8)

recommended_movie = movie_df[movie_df['id'].isin(recommended_movie_ids)]
for row in recommended_movie.itertuples():
    print(row.title, ':', row.genres)

"""Output Rekomendasi Movie untuk User

- **Model** berhasil memprediksi rekomendasi movie untuk 1 user acak dari dataset (`userId = 66`).
- **Bagian pertama** menampilkan 5 movie terbaik yang sebelumnya sudah dirating tinggi oleh user.
- **Bagian kedua** menampilkan 10 movie baru yang direkomendasikan oleh model berdasarkan prediksi skor tertinggi.

Penjelasan Output:
- **"Movie with high ratings from user"**:
  - Daftar film favorit user berdasarkan data historis rating yang tinggi.
- **"Top 10 movie recommendation"**:
  - Daftar film baru yang belum pernah ditonton user, tetapi diprediksi akan disukai berdasarkan hasil training model.

Ini membuktikan bahwa model rekomendasi dapat menghasilkan prediksi personalisasi berdasarkan histori user!


"""